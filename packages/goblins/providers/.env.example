# LiteLLM Gateway Configuration
LITELLM_BASE_URL=http://litellm:4000
LITELLM_API_KEY=dummy

# Default Model Selection
DEFAULT_MODEL=gpt-4-turbo

# Fallback Models (comma-separated, in priority order)
# These will be tried in sequence if the primary model fails
FALLBACK_MODELS=gemini-pro,deepseek-chat,ollama-local

# Telemetry & Observability
TELEMETRY_ENABLED=true
TRACK_COST=true

# OpenTelemetry Configuration
OTEL_EXPORTER_OTLP_ENDPOINT=http://jaeger:4318/v1/traces

# Request Configuration
REQUEST_TIMEOUT=60000
MAX_TOKENS=4096
TEMPERATURE=0.7
