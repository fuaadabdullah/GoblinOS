---
title: GoblinOS
type: reference
project: GoblinOS
status: reviewed
owner: GoblinOS
goblin_name: GoblinOS Platform
---

**AI Agents & Automation System** - Monorepo tooling, agents, and workspace health automation for the ForgeMonorepo.

## ğŸš€ Quick Start

```bash
cd ForgeMonorepo/GoblinOS
pnpm install
pnpm build
```

## Goblin CLI (quick run)

GoblinOS exposes a declarative set of "goblins" in `GoblinOS/goblins.yaml`. A small helper script will be provided to list and safely run these goblins. Example usage (placeholder until `goblin-cli` is added):

```bash
# List available goblins
bash GoblinOS/goblin-cli.sh list

# Dry-run a goblin (safe)
bash GoblinOS/goblin-cli.sh run --dry <goblin-id>

# Execute (owners only for destructive tasks)
bash GoblinOS/goblin-cli.sh run <goblin-id>
```

Note: A lightweight `goblin-cli` scaffold will be added soon to validate and safely execute entries in `GoblinOS/goblins.yaml`.

**Before opening a PR:**

```bash
pnpm lint:fix && pnpm test:coverage && pnpm build
```

## ğŸ³ Ollama Setup (Required for AI Features)

GoblinOS uses Ollama for local LLM inference. To activate Ollama:

### Option 1: Docker (Recommended)

```bash
# In the overmind package directory
cd packages/goblins/overmind
docker-compose up -d ollama
```

This starts Ollama on port 11435 (to avoid conflicts).

### Option 2: Local Installation

```bash
# Install Ollama (macOS)
brew install ollama

# Start Ollama service
ollama serve

# Pull a model (in another terminal)
ollama pull qwen2.5:3b
```

### Verify Ollama is Running

```bash
curl http://localhost:11434/api/tags
# Should return JSON with available models
```

### Environment Configuration

Copy and configure your environment:

```bash
cp .env.example .env
# Edit .env with your API keys and Ollama settings
```

For Overmind development, set a local LLM endpoint via env vars. Choose one:

```bash
# Option A: Direct connection to Ollama (OpenAI-compatible path appended automatically)
export OLLAMA_BASE_URL=http://localhost:11434
export OLLAMA_DEFAULT_MODEL=llama3.2

# Optional: override fallbacks (comma-separated)
# export FALLBACK_MODELS=llama3.2
```

```bash
# Option B: Direct connection to DeepSeek (OpenAI-compatible)
export DEEPSEEK_API_KEY=sk-your-key
# Optional override (defaults to https://api.deepseek.com)
# export DEEPSEEK_BASE_URL=https://api.deepseek.com
export DEEPSEEK_DEFAULT_MODEL=deepseek-r1
```

```bash
# Option C: Direct connection to OpenAI
export OPENAI_API_KEY=sk-your-key
# Optional override (defaults to https://api.openai.com)
# export OPENAI_BASE_URL=https://api.openai.com
export OPENAI_DEFAULT_MODEL=gpt-4-turbo
```

Then verify connectivity:

```bash
node GoblinOS/examples/ollama-connection-test.js "Say hello from Ollama."
# or
node GoblinOS/examples/deepseek-connection-test.js "Say hello from DeepSeek."
# or (via OpenAI)
# use goblin ask or the providers directly since OpenAI uses the standard /v1 API
```

If `OLLAMA_BASE_URL` is not set, GoblinOS falls back to a LiteLLM gateway
via `LITELLM_BASE_URL`.

## ğŸ—ï¸ Architecture

```text
GoblinOS/
â”œâ”€â”€ ğŸ“¦ packages/
â”‚   â”œâ”€â”€ ğŸ–¥ï¸ cli/                    # Goblin CLI entry point
â”‚   â””â”€â”€ ğŸ¤– goblins/
â”‚       â”œâ”€â”€ ğŸ“ quillwarden/         # Obsidian vault automation
â”‚       â”œâ”€â”€ ğŸ—ï¸ repo-bootstrap/      # Repository scaffolding
â”‚       â””â”€â”€ ğŸ¥ workspace-health/    # Health checks and smoke tests
â”œâ”€â”€ ğŸ“‹ .changeset/              # Release changesets
â”œâ”€â”€ âš™ï¸ tsconfig.build.json      # TypeScript project references
â”œâ”€â”€ ğŸ§ª vitest.config.ts         # Test configuration
â””â”€â”€ ğŸ”§ biome.json               # Lint/format configuration
```

## ğŸ›¡ï¸ Production Tooling

GoblinOS is hardened with:

- âœ… **TypeScript Project References** - Fast incremental builds
- âœ… **Vitest + v8 Coverage** - 90% code coverage enforced
- âœ… **Biome** - Fast linting & formatting
- âœ… **Changesets** - Semantic versioning & changelogs
- âœ… **npm Provenance** - Signed, verifiable releases
- âœ… **CI Matrix** - Node 20 & 22
- âœ… **Security Scanning** - CodeQL, OpenSSF Scorecard, SBOM
- âœ… **Dependency Updates** - Automated via Renovate
- âœ… **Architecture Guardrails** - dependency-cruiser

## ğŸ“š Documentation

- **[ğŸ§­ SOURCE_OF_TRUTH.md](./SOURCE_OF_TRUTH.md)** - Canonical map of mission, architecture, workflows, and runbooks
- **[ğŸ“‹ COMMANDS.md](./docs/COMMANDS.md)** - Quick command reference
- **[âš™ï¸ SETUP.md](./docs/SETUP.md)** - Installation & troubleshooting
- **[ğŸ›¡ï¸ PRODUCTION_HARDENING.md](./docs/PRODUCTION_HARDENING.md)** - Deep dive on tooling
- **[ğŸ“Š HARDENING_SUMMARY.md](./docs/HARDENING_SUMMARY.md)** - What was implemented

## ğŸ› ï¸ Development

### Scripts

| Command | Description |
|---------|-------------|
| `pnpm dev` | ğŸƒ Run CLI in dev mode |
| `pnpm check` | ğŸ” Type check (no emit) |
| `pnpm build` | ğŸ—ï¸ Incremental build |
| `pnpm test` | ğŸ§ª Run tests |
| `pnpm test:coverage` | ğŸ“Š Tests + coverage |
| `pnpm lint` | ğŸ” Check for errors |
| `pnpm lint:fix` | ğŸ”§ Auto-fix errors |
| `pnpm changeset` | ğŸ“ Create release changeset |

### CI/CD

**On PR:**

- ğŸ” Type check (Node 20 & 22)
- ğŸ§¹ Lint (Biome)
- ğŸ§ª Test with coverage
- ğŸ—ï¸ Build
- ğŸ”’ Security scans

**On main:**

- âœ… All PR checks
- ğŸ›¡ï¸ OpenSSF Scorecard
- ğŸ“¦ SBOM generation
- ğŸš€ Automated releases (via Changesets)

## ğŸ“¦ Releasing

Releases are automated:

1. ğŸ“ Create changeset: `pnpm changeset`
2. ğŸ’¾ Commit and push
3. ğŸ”€ Merge the "Version Packages" PR
4. ğŸ“¤ Packages are published to npm with provenance

## ğŸ“„ License

See workspace root LICENSE.
